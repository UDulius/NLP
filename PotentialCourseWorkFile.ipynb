{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYW67jlKkJX+jn9OtjdcAR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UDulius/NLP/blob/main/PotentialCourseWorkFile.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FttbV6435Hlg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "twenty_train = fetch_20newsgroups(subset='train',  categories=categories, shuffle=True, random_state=42)\n",
        "twenty_test = fetch_20newsgroups(subset='test',  categories=categories, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = input('type your student number')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kv7bh1Rm5YqW",
        "outputId": "4e6f6318-eeff-4671-e36b-cb0130cf8385"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "type your student number29008326\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=divmod(int(index),4)\n",
        "yourdata1=x[1]\n",
        "y=divmod(int(index),3)\n",
        "yourdata2=y[1]\n",
        "\n",
        "print('This is your data set index ----> (', x[1], y[1], ')')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EvYeJGY5cZD",
        "outputId": "06e622fa-f7d3-40ac-ec24-848ed23c499b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is your data set index ----> ( 2 0 )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data1= twenty_train.target_names[x[1]]\n",
        "data2= twenty_train.target_names[y[1]]\n",
        "categories1=[data1,data2]\n",
        "print(categories1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zg4c8er75ti3",
        "outputId": "7d31a537-029b-4538-cd6e-b4b60f92d8c7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sci.med', 'alt.atheism']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbmgec_q5y0G",
        "outputId": "d8262dd4-41b3-4554-bef5-bb1073b9e8bb"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sci.med\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from tqdm import tqdm\n",
        "from nltk.corpus import stopwords\n",
        "englishStopwords = stopwords.words('english')\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TVaYJL69O0C",
        "outputId": "c1af38f6-9b4e-4aa2-b50d-f04456d649ec"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def findLemma(word):\n",
        "  lemma = wordnet.morphy(word)\n",
        "  if lemma is not None:\n",
        "    return lemma\n",
        "  else:\n",
        "    return word\n",
        "\n",
        "def findStems(word):\n",
        "  stem = stemmer.stem(word)\n",
        "  if stem is not None:\n",
        "    return stem\n",
        "  else:\n",
        "    return word\n",
        "\n",
        "def processText(text, lemma=False, gram=1, rmStop=True):\n",
        "  text = re.sub(r'(http|https)?:\\/\\/(\\w|/.|\\/|\\?|\\=|\\%|\\%)*\\b|@\\w+|#', '', text, flags=re.MULTILINE) #Deletes any URLs, hashtags, and @text\n",
        "  tokens = word_tokenize(text) \n",
        "  whitelist = [\"n't\", \"not\", \"no\"]\n",
        "  new_tokens = []\n",
        "  stoplist = englishStopwords if rmStop else []\n",
        "  for i in tokens:\n",
        "    i = i.lower()\n",
        "    if i.isalpha() and (i not in stoplist or i in whitelist):\n",
        "      if lemma: i = findLemma(i)\n",
        "      new_tokens.append(i)\n",
        "  del tokens\n",
        "\n",
        "  if gram<=1:\n",
        "    return new_tokens\n",
        "  else:\n",
        "    return [' '.join(i) for i in nltk.ngrams(new_tokens, gram)]\n"
      ],
      "metadata": {
        "id": "xZJ1y7ZB-3mL"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getTags(text):\n",
        "  token = word_tokenize(text)\n",
        "  token = [l.lower() for l in token]\n",
        "  train_tags = nltk.pos_tag(token)\n",
        "  return [i[1] for i in train_tags]"
      ],
      "metadata": {
        "id": "zPuFVC9T_X2o"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = twenty_train.data[1]\n",
        "print(processText(dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmQ_fVCEHCGn",
        "outputId": "78a5954f-bd8f-4cc4-c95f-e0554eb555df"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['aniruddha', 'deglurkar', 'subject', 'help', 'splitting', 'trimming', 'region', 'along', 'mesh', 'organization', 'university', 'kentucky', 'dept', 'math', 'sciences', 'lines', 'hi', 'problem', 'hope', 'help', 'solve', 'background', 'problem', 'rectangular', 'mesh', 'uv', 'domain', 'mesh', 'mapping', 'bezier', 'patch', 'area', 'domain', 'inside', 'trimming', 'loop', 'rendered', 'trimming', 'loop', 'set', 'bezier', 'curve', 'segments', 'sake', 'notation', 'mesh', 'made', 'cells', 'problem', 'trimming', 'area', 'split', 'individual', 'smaller', 'cells', 'bounded', 'trimming', 'curve', 'segments', 'cell', 'wholly', 'inside', 'area', 'output', 'whole', 'else', 'trivially', 'rejected', 'body', 'know', 'thiss', 'done', 'algo', 'somewhere', 'help', 'would', 'appreciated', 'thanks', 'ani', 'get', 'irritated', 'human', 'stay', 'cool', 'divine']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(getTags(dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUqawgb-H5a0",
        "outputId": "b49a8ef3-f292-4d37-ec7e-ab18068eb0c9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['IN', ':', 'NN', 'NN', 'NN', '(', 'JJ', 'NN', 'NN', ')', 'NN', ':', 'NN', ':', 'VBG', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', ':', 'NN', 'IN', 'NN', ',', 'NN', '.', 'IN', 'NN', 'NNS', 'NNS', ':', 'CD', 'NN', ',', 'NN', 'VBP', 'DT', 'NN', ',', 'NN', 'VBP', 'DT', 'IN', 'DT', 'NNP', 'POS', 'MD', 'VB', 'PRP', 'VB', '.', 'NN', 'IN', 'DT', 'NN', ':', 'NN', 'VBP', 'DT', 'JJ', 'NN', 'IN', 'DT', 'JJ', 'NN', ',', 'VBP', 'DT', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'DT', 'CD', 'NN', 'NN', 'IN', 'CD', '.', 'DT', 'NN', 'IN', 'DT', 'NN', 'WDT', 'VBZ', 'IN', 'DT', 'NN', 'NN', 'VBD', 'TO', 'VB', 'VBN', '.', 'DT', 'VBG', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'CD', 'NN', 'NN', 'NNS', '.', 'IN', 'DT', 'NN', 'IN', 'NN', ':', 'DT', 'NN', 'VBZ', 'VBN', 'IN', 'IN', 'NNS', '.', 'PRP$', 'NN', 'VBZ', 'DT', ':', 'DT', 'VBG', 'NN', 'VBZ', 'TO', 'VB', 'VBN', 'RP', 'IN', 'JJ', 'JJR', 'NNS', 'VBN', 'IN', 'DT', 'VBG', 'NN', 'NNS', '.', 'IN', 'DT', 'NN', 'VBZ', 'RB', 'IN', 'DT', 'NN', ':', 'RB', 'PRP', 'VBZ', 'NN', 'IN', 'DT', 'NN', ',', 'VB', 'PRP', 'VBZ', 'RB', 'VBN', '.', 'VBZ', 'DT', 'NN', 'VB', 'WRB', 'JJ', 'MD', 'VB', 'VBN', ',', 'CC', 'VBZ', 'RB', 'DT', 'NN', '.', 'RB', 'IN', 'VBG', 'DT', '.', 'DT', 'NN', 'MD', 'VB', 'VBN', '.', 'NNS', ',', 'NN', '.', ':', 'TO', 'VB', 'JJ', 'VBZ', 'JJ', ',', 'TO', 'VB', 'JJ', ',', 'NN', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VXtGg_tVJ-wn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}